{
    "responses": {
        "question1": "Cane, Gait, Drywall",
        "question2": "Usually. Even tasks the can be fully inferred from the data, the video is needed to calibrate oneself to the data. An alternative to providing video would be to provide some example data with events marked (e.g. for gait).",
        "question3": "Events that the data are very obviously correlated with. For noisy data when events cannot be inferred directly, it still might help refine the start/stop events while the video is used to identify the event.",
        "question4": "It would have been faster/easier if I could e.g. watch the video in real time, add \"key frames\" when I saw event starts/stops (e.g. push space bar), and classify these pre-marked chunks of time and refine them ex-post. Alternatively, I might push e.g. \"1\" for the duration of the action, and release when it was completed to roughly identify an event, then refine later. Maybe this tool already supports it. At any rate, I never felt like I had a good system going, I was always fighting the interface, though this might be mitigated with more practice with VAT."
    }
}